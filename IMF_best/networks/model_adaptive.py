# æ–‡ä»¶å: imf_model.py
# (ä¸»è¦ä¿®æ”¹ç‚¹: FrameDecoder)

import torch
import torch.nn as nn
import argparse

# --- æ ¸å¿ƒä¾èµ–å¯¼å…¥ ---
from utils.modules import (
    DownConvResBlock, ResBlock, UpConvResBlock, ConvResBlock, 
)
# å¯¼å…¥æ‰€æœ‰æ³¨æ„åŠ›æ¨¡å—ï¼ŒåŒ…æ‹¬æ–°å¢çš„ SwinTransformerBlock
from utils.attention_modules import AttentionLayerFactory, ThreeCrossAttentionLayerFactory
from utils.lia_resblocks import StyledConv,EqualConv2d,EqualLinear
# ... LatentTokenEncoder, DenseFeatureEncoder, LatentTokenDecoder çš„ä»£ç ä¿æŒä¸å˜ ...
class DenseFeatureEncoder(nn.Module):
    # ... (ä»£ç ä¸å˜) ...
    def __init__(self, in_channels=3, output_channels=[64, 128, 256, 512, 512, 512], initial_channels=32, dm=512):
        super().__init__()
        self.initial_conv = nn.Sequential(
            nn.Conv2d(in_channels, initial_channels, kernel_size=7, stride=1, padding=3),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        self.down_blocks = nn.ModuleList()
        current_channels = initial_channels
        for out_channels in output_channels:
            if out_channels==32:continue
            self.down_blocks.append(DownConvResBlock(current_channels, out_channels))
            current_channels = out_channels

        # Equal convolution and linear layers
        self.equalconv = EqualConv2d(output_channels[-1], output_channels[-1], kernel_size=3, stride=1, padding=1)
        self.linear_layers = nn.ModuleList([EqualLinear(output_channels[-1], output_channels[-1]) for _ in range(4)])
        self.final_linear = EqualLinear(output_channels[-1], dm)
        self.activation = nn.LeakyReLU(0.2)

    def forward(self, x):
        features = []
        x = self.initial_conv(x)
        features.append(x)
        for block in self.down_blocks:
            x = block(x)
            features.append(x)
        x = x.view(x.size(0), x.size(1), -1).mean(dim=2)
        # Apply linear layers
        for linear_layer in self.linear_layers:
            x = self.activation(linear_layer(x))
        # Final linear layer
        x = self.final_linear(x)
        return features[::-1], x

class LatentTokenEncoder(nn.Module):
    def __init__(self, initial_channels=64, output_channels=[64, 128, 256, 512, 512, 512], dm=32):
        super(LatentTokenEncoder, self).__init__()

        # Initial convolution followed by LeakyReLU activation
        self.conv1 = nn.Conv2d(3, initial_channels, kernel_size=3, stride=1, padding=1)
        self.activation = nn.LeakyReLU(0.2)

        # Dynamically create ResBlocks
        self.res_blocks = nn.ModuleList()
        in_channels = initial_channels
        for out_channels in output_channels:
            self.res_blocks.append(ResBlock(in_channels, out_channels))
            in_channels = out_channels

        # Equal convolution and linear layers
        self.equalconv = EqualConv2d(output_channels[-1], output_channels[-1], kernel_size=3, stride=1, padding=1)
        self.linear_layers = nn.ModuleList([EqualLinear(output_channels[-1], output_channels[-1]) for _ in range(4)])
        self.final_linear = EqualLinear(output_channels[-1], dm)

    def forward(self, x):
        # Initial convolution and activation
        x = self.activation(self.conv1(x))
        
        # Apply ResBlocks
        for res_block in self.res_blocks:
            x = res_block(x)
        
        # Apply equalconv
        x = self.equalconv(x)
        
        # Global average pooling
        x = x.view(x.size(0), x.size(1), -1).mean(dim=2)
        
        # Apply linear layers
        for linear_layer in self.linear_layers:
            x = self.activation(linear_layer(x))
            
        
        # Final linear layer
        x = self.final_linear(x)
        
        return x

class LatentTokenDecoder(nn.Module):
    def __init__(self, latent_dim=32, const_dim=32):
        super().__init__()
        # Constant input for the decoder
        self.const = nn.Parameter(torch.randn(1, const_dim, 4, 4))
        
        # StyleConv layers
        self.style_conv_layers = nn.ModuleList([
            StyledConv(const_dim, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim, upsample=True),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim, upsample=True),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 256, 3, latent_dim, upsample=True),
            StyledConv(256, 256, 3, latent_dim),
            StyledConv(256, 256, 3, latent_dim),
            StyledConv(256, 128, 3, latent_dim, upsample=True),
            StyledConv(128, 128, 3, latent_dim),
            StyledConv(128, 128, 3, latent_dim)  
        ])

    def forward(self, t):
        # Repeat constant input for batch size
        x = self.const.repeat(t.shape[0], 1, 1, 1)
        #import pdb;pdb.set_trace()
        # Store feature maps
        m1, m2, m3, m4 = None, None, None, None
        # Apply style convolution layers
        for i, layer in enumerate(self.style_conv_layers):
            x = layer(x, t)
            
            if i == 3:
                m1 = x
            elif i == 6:
                m2 = x
            elif i == 9:
                m3 = x
            elif i == 12:
                m4 = x
        
        # Return the feature maps in reverse order
        return m1, m2, m3, m4

class IdTokenDecoder(nn.Module):
    def __init__(self, latent_dim=512, const_dim=512):
        super().__init__()
        # Constant input for the decoder
        self.const = nn.Parameter(torch.randn(1, const_dim, 4, 4))
        
        # StyleConv layers
        self.style_conv_layers = nn.ModuleList([
            StyledConv(const_dim, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim, upsample=True),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim, upsample=True),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 256, 3, latent_dim, upsample=True),
            StyledConv(256, 256, 3, latent_dim),
            StyledConv(256, 256, 3, latent_dim),
            StyledConv(256, 128, 3, latent_dim, upsample=True),
            StyledConv(128, 128, 3, latent_dim),
            StyledConv(128, 128, 3, latent_dim)  
        ])

    def forward(self, t):
        # Repeat constant input for batch size
        x = self.const.repeat(t.shape[0], 1, 1, 1)
        #import pdb;pdb.set_trace()
        # Store feature maps
        m1, m2, m3, m4 = None, None, None, None
        # Apply style convolution layers
        for i, layer in enumerate(self.style_conv_layers):
            x = layer(x, t)
            
            if i == 3:
                m1 = x
            elif i == 6:
                m2 = x
            elif i == 9:
                m3 = x
            elif i == 12:
                m4 = x
        
        # Return the feature maps in reverse order
        return m1, m2, m3, m4

# ============================================================================
# ä¸»æ¨¡å‹ (æœ€ç»ˆç®€åŒ–ç‰ˆ)
# ============================================================================
class FrameDecoder(nn.Module):
    def __init__(self, args, feature_dims, spatial_dims):
        super().__init__()
        self.args = args
        
        feature_dims_rev = feature_dims[::-1]
        spatial_dims_rev = spatial_dims[::-1]

        self.upconv_blocks = nn.ModuleList([
            UpConvResBlock(feature_dims_rev[i], feature_dims_rev[i+1]) for i in range(len(feature_dims_rev) - 1)
        ])
        self.resblocks = nn.ModuleList([
            ConvResBlock(feature_dims_rev[i+1]*2, feature_dims_rev[i+1]) for i in range(len(feature_dims_rev) - 1)
        ])
        
        self.transformer_blocks = nn.ModuleList()
        print("ğŸ”§ æ­£åœ¨é€šè¿‡å·¥å‚æ„å»ºè§£ç å™¨ä¸­çš„ç»Ÿä¸€è‡ªæ³¨æ„åŠ›å±‚:")
        for i in range(len(spatial_dims_rev) - 1):
            s_dim = spatial_dims_rev[i+1]
            f_dim = feature_dims_rev[i+1]
            self.transformer_blocks.append(
                AttentionLayerFactory(args=args, dim=f_dim, resolution=(s_dim, s_dim))
            )

        self.final_conv = nn.Sequential(
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(feature_dims_rev[-1], 3*4, kernel_size=3, padding=1),
            nn.PixelShuffle(upscale_factor=2),
            nn.Sigmoid()
        )

    def forward(self, features_align):
        x = features_align[0]
        #import pdb;pdb.set_trace()
        for i in range(len(self.upconv_blocks)):
            x = self.upconv_blocks[i](x)
            x = torch.cat([x, features_align[i + 1]], dim=1)
            x = self.resblocks[i](x)
            x = self.transformer_blocks[i](x)
        return self.final_conv(x)

class IdAdaptive(nn.Module):
    def __init__(self, dim_mot=32, dim_app=512, depth=4):
        super().__init__()
        self.in_layer = EqualLinear(dim_app+dim_mot, dim_app)
        self.linear_layers = nn.ModuleList([EqualLinear(dim_app, dim_app) for _ in range(depth)])
        self.final_linear = EqualLinear(dim_app, dim_mot)  # æˆæ’³åš­ shift + scale
        self.activation = nn.LeakyReLU(0.2)
        self.scale_activation = nn.Sigmoid()  # é—„æ„¬åŸ— scale é¦?[0,1]

    def modulate(self, x, shift, scale) -> torch.Tensor:
        # x: (B, dim_mot)
        # shift/scale: (B, dim_mot)
        return x * scale + shift

    def forward(self, mot, app):
        """
        mot: (B, dim_mot)
        app: (B, dim_app)
        """
        x = torch.cat((mot, app), dim=-1)
        x = self.in_layer(x)
        for linear_layer in self.linear_layers:
            x = self.activation(linear_layer(x))
        # éˆâ‚¬éšåºç«´çå‚ç·­é‘?shift éœ?scale
        out = self.final_linear(x)
        #scale = self.scale_activation(scale)  # é—„æ„¬åŸ— scale
        #out = self.modulate(mot, shift, scale)
        return out

class IMFModel(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.feature_dims = [32, 64, 128, 256, 512, 512]
        self.motion_dims = self.feature_dims
        self.spatial_dims = [256, 128, 64, 32, 16, 8]

        self.dense_feature_encoder = DenseFeatureEncoder(output_channels=self.feature_dims)
        self.latent_token_encoder = LatentTokenEncoder(initial_channels=64, output_channels=[128, 256, 512, 512, 512])
        self.latent_token_decoder = LatentTokenDecoder()
        #self.id_token_decoder = IdTokenDecoder()
        
        self.frame_decoder = FrameDecoder(args, self.feature_dims, self.spatial_dims)

        self.implicit_motion_alignment = nn.ModuleList()
        print("ğŸ”§ æ­£åœ¨é€šè¿‡å·¥å‚æ„å»ºå¯¹é½é˜¶æ®µçš„ç»Ÿä¸€äº¤å‰æ³¨æ„åŠ›å±‚:")
        for dim, s_dim in zip(self.feature_dims[::-1], self.spatial_dims[::-1]):
            self.implicit_motion_alignment.append(
                ThreeCrossAttentionLayerFactory(args=args, dim=dim, resolution=(s_dim, s_dim))
            )
        self.adapt = IdAdaptive()

    def decode(self, A, B, C):
        num_levels = len(self.spatial_dims)
        aligned_features = [None] * num_levels
        attention_map = None # åˆå§‹åŒ– attention_map ä¸º None
        for i in range(num_levels):
            attention_block = self.implicit_motion_alignment[i]
            if attention_block.is_standard_attention:
                aligned_feature, attention_map = attention_block.coarse_warp(A[i], B[i], C[i])
                aligned_features[i] = aligned_feature
            else:
                aligned_feature = attention_block.fine_warp(C[i], attn=attention_map)
                aligned_features[i] = aligned_feature
        output_frame = self.frame_decoder(aligned_features)
        return output_frame
    

    def app_encode(self, x):
        f_r, id = self.dense_feature_encoder(x)
        return f_r, id
    
    def mot_encode(self, x):
        mot_latent = self.latent_token_encoder(x)
        return mot_latent
    
    def mot_decode(self, x):
        mot_map = self.latent_token_decoder(x)
        return mot_map
    
    def id_adapt(self, t, id):
        return self.adapt(t, id)
    
    def forward(self, x_current, x_reference):
        f_r, i_r = self.app_encode(x_reference)
        t_r = self.mot_encode(x_reference)
        t_c = self.mot_encode(x_current)
        ta_r = self.adapt(t_r, i_r)
        ta_c = self.adapt(t_c, i_r)
        ma_r = self.mot_decode(ta_r)
        ma_c = self.mot_decode(ta_c)
        output_frame = self.decode(ma_c, ma_r, f_r)
        return output_frame

import torch
import torch.nn as nn
import time
from collections import defaultdict
import numpy as np

# ============================================================================
# Part 1: æ–°å¢çš„æ¨¡å—è®¡æ—¶å·¥å…·ç±»
# ============================================================================
class ModuleTimer:
    """
    ä¸€ä¸ªä½¿ç”¨ PyTorch Hooks æ¥ä¸º nn.Module è®¡æ—¶çš„å·¥å…·ç±»ã€‚
    å®ƒä¸ä¿®æ”¹æ¨¡å‹æ¥å£ï¼Œå¹¶ä¸”èƒ½ç²¾ç¡®æµ‹é‡ CUDA ä¸Šçš„æ‰§è¡Œæ—¶é—´ã€‚
    """
    def __init__(self, modules_to_time, device):
        self.modules_to_time = modules_to_time
        self.device = device
        self.timings = defaultdict(list)
        self.start_events = {}

        self._register_hooks()

    def _start_timing(self, name):
        """è®°å½•å¼€å§‹æ—¶é—´ç‚¹ã€‚"""
        if self.device.type == 'cuda':
            # ç¡®ä¿ä¹‹å‰çš„CUDAæ“ä½œå®Œæˆ
            torch.cuda.synchronize()
            start_event = torch.cuda.Event(enable_timing=True)
            start_event.record()
            self.start_events[name] = start_event
        else:
            self.start_events[name] = time.time()

    def _stop_timing(self, name):
        """è®°å½•ç»“æŸæ—¶é—´ç‚¹å¹¶è®¡ç®—è€—æ—¶ã€‚"""
        if name not in self.start_events:
            return  # å¦‚æœæ²¡æœ‰å¼€å§‹äº‹ä»¶ï¼Œåˆ™è·³è¿‡

        start_event_or_time = self.start_events[name]

        if self.device.type == 'cuda':
            end_event = torch.cuda.Event(enable_timing=True)
            end_event.record()
            # ç¡®ä¿ç»“æŸäº‹ä»¶ä¹Ÿå®Œæˆ
            torch.cuda.synchronize()
            # elapsed_time è¿”å›æ¯«ç§’ (ms)
            duration_ms = start_event_or_time.elapsed_time(end_event)
            self.timings[name].append(duration_ms)
        else:
            end_time = time.time()
            duration_ms = (end_time - start_event_or_time) * 1000
            self.timings[name].append(duration_ms)

        # æ¸…ç†å·²ä½¿ç”¨çš„äº‹ä»¶
        del self.start_events[name]

    def _make_pre_hook(self, name):
        """åˆ›å»ºä¸€ä¸ªå‰å‘ä¼ æ’­å‰çš„é’©å­å‡½æ•°ã€‚"""
        def pre_hook(module, input):
            self._start_timing(name)
        return pre_hook

    def _make_forward_hook(self, name):
        """åˆ›å»ºä¸€ä¸ªå‰å‘ä¼ æ’­åçš„é’©å­å‡½æ•°ã€‚"""
        def forward_hook(module, input, output):
            self._stop_timing(name)
        return forward_hook

    def _register_hooks(self):
        """ä¸ºæ‰€æœ‰æŒ‡å®šçš„æ¨¡å—æ³¨å†Œé’©å­ã€‚"""
        for name, module in self.modules_to_time.items():
            if isinstance(module, nn.ModuleList) or isinstance(module, list):
                 # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸ªå­æ¨¡å—æ³¨å†Œé’©å­
                for i, sub_module in enumerate(module):
                    sub_name = f"{name}_{i}"
                    sub_module.register_forward_pre_hook(self._make_pre_hook(sub_name))
                    sub_module.register_forward_hook(self._make_forward_hook(sub_name))
            else:
                 module.register_forward_pre_hook(self._make_pre_hook(name))
                 module.register_forward_hook(self._make_forward_hook(name))

    def reset(self):
        """é‡ç½®è®¡æ—¶å™¨ï¼Œæ¸…é™¤æ‰€æœ‰è®°å½•çš„æ—¶é—´ã€‚"""
        self.timings.clear()

    def summary(self):
        """æ‰“å°è®¡æ—¶ç»“æœçš„æ‘˜è¦è¡¨æ ¼ã€‚"""
        if not self.timings:
            print("No timings recorded.")
            return

        print("\n" + "="*80)
        print("ğŸ“Š å¹³å‡è¿è¡Œé€Ÿåº¦åˆ†æ (ms)")
        print("="*80)
        print(f"{'æ¨¡å—å':<40} {'å¹³å‡è€—æ—¶ (ms)':<20} {'æ€»è€—æ—¶ (ms)':<20}")
        print("-"*80)

        # ç”¨äºèšåˆ alignment çš„æ€»æ—¶é—´
        alignment_times = []
        
        for name, times in sorted(self.timings.items()):
            if "alignment" in name:
                alignment_times.extend(times)

            avg_time = np.mean(times)
            total_time = np.sum(times)
            
            # åªæ‰“å°éèšåˆçš„ç»“æœ
            if "alignment_" in name:
                print(f"  - {name:<36} {avg_time:<20.3f} {total_time:<20.3f}")
            elif "alignment" not in name:
                 print(f"{name:<40} {avg_time:<20.3f} {total_time:<20.3f}")

        # è®¡ç®—å¹¶æ‰“å°èšåˆçš„ alignment æ—¶é—´
        if alignment_times:
            avg_alignment_time = np.mean(alignment_times) * len(self.modules_to_time['alignment'])
            total_alignment_time = np.sum(alignment_times)
            print("-"*80)
            print(f"{'alignment (èšåˆ)':<40} {avg_alignment_time:<20.3f} {total_alignment_time:<20.3f}")

        print("="*80)
# ============================================================================
# Part 3: è®­ç»ƒæ˜¾å­˜æµ‹è¯•ä¸»æ‰§è¡Œå—
# ============================================================================
def main():
    # --- 1. å‚æ•°é…ç½® ---
    parser = argparse.ArgumentParser(description="IMFModel Training Memory Test")
    parser.add_argument('--latent_dim', type=int, default=32, help='Dimension of the latent tokens.')
    # é˜ˆå€¼è®¾ä¸º 16*16=256ï¼Œè¿™æ · 8x8 å’Œ 16x16 å±‚ä¼šç”¨æ ‡å‡†æ³¨æ„åŠ›
    parser.add_argument('--swin_res_threshold', type=int, default=128, help='Resolution threshold to switch to Swin Attention.')
    parser.add_argument('--num_heads', type=int, default=8, help='Number of attention heads.')
    parser.add_argument('--window_size', type=int, default=8, help='Window size for Swin Attention.')
    parser.add_argument('--drop_path', type=float, default=0.1, help='Stochastic depth rate for Swin.')
    parser.add_argument('--low_res_depth', type=int, default=2, help='Number of TransformerBlocks for low-res features.')
    parser.add_argument('--batch_size', type=int, default=2, help='Batch size for training.')
    parser.add_argument('--img_size', type=int, default=256, help='Input image size.')
    
    args, _ = parser.parse_known_args()

    # --- 2. æ¨¡å‹åˆå§‹åŒ– (ä¿æŒä¸å˜) ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = IMFModel(args).to(device)
    model.eval()

    # --- 3. å®šä¹‰è¦è®¡æ—¶çš„æ¨¡å— (ä¿æŒä¸å˜) ---
    modules_to_time = {
        'dense_feature_encoder': model.dense_feature_encoder,
        'latent_token_encoder': model.latent_token_encoder,
        'latent_token_decoder': model.latent_token_decoder,
        'alignment': model.implicit_motion_alignment, # è¿™æ˜¯ä¸€ä¸ª ModuleList
        'frame_decoder': model.frame_decoder,

    }

    # --- 4. åˆå§‹åŒ–è®¡æ—¶å™¨å¹¶æ³¨å†Œé’©å­ (ä¿æŒä¸å˜) ---
    print("\n" + "="*50)
    print("â±ï¸  æ­£åœ¨åˆå§‹åŒ–æ¨¡å—è®¡æ—¶å™¨å¹¶æ³¨å†Œé’©å­...")
    timer = ModuleTimer(modules_to_time, device)
    print("âœ… é’©å­æ³¨å†Œå®Œæˆ!")
    print("="*50)

    # --- 5. å‡†å¤‡è¾“å…¥æ•°æ® (ä¿æŒä¸å˜) ---
    batch_size = 1
    img_size = 256
    x_current = torch.randn(batch_size, 3, img_size, img_size).to(device)
    x_reference = torch.randn(batch_size, 3, img_size, img_size).to(device)
    
    # --- 6. è¿è¡Œé€Ÿåº¦æµ‹è¯• ---
    with torch.no_grad():
        # --- æ¨¡å‹é¢„çƒ­ ---
        print("\n" + "-"*50)
        print("æ­£åœ¨é¢„çƒ­æ¨¡å‹ (è¿è¡Œ10æ¬¡)...")
        for _ in range(10):
            _ = model(x_current, x_reference)
        print("é¢„çƒ­å®Œæˆã€‚")
        print("-"*50)

        # æ¸…é™¤é¢„çƒ­æœŸé—´çš„è®¡æ—¶æ•°æ®
        timer.reset()

        # --- æ€§èƒ½æµ‹è¯• ---
        num_runs = 1000
        print(f"\næ­£åœ¨ç²¾ç¡®æµ‹è¯• {num_runs} æ¬¡è¿è¡Œçš„æ¨¡å—å’Œæ€»é€Ÿåº¦...")
        
        # <--- æ–°å¢ï¼šä¸ºæ€»æ—¶é—´è®¡æ—¶åšå‡†å¤‡ ---
        if device.type == 'cuda':
            torch.cuda.synchronize()
        start_time_total = time.time()
        # <--- æ–°å¢ç»“æŸ ---

        for _ in range(num_runs):
            _ = model(x_current, x_reference)
            
        # <--- æ–°å¢ï¼šè®°å½•æ€»æ—¶é—´ç»“æŸå¹¶è®¡ç®— ---
        if device.type == 'cuda':
            torch.cuda.synchronize()
        end_time_total = time.time()
        
        total_elapsed_time = end_time_total - start_time_total
        avg_time_per_run_ms = (total_elapsed_time / num_runs) * 1000
        fps = num_runs / total_elapsed_time
        # <--- æ–°å¢ç»“æŸ ---
            
        print("âœ… é€Ÿåº¦æµ‹è¯•å®Œæˆ!")

    # --- 7. æ‰“å°æ¨¡å—è®¡æ—¶ç»“æœ ---
    timer.summary()

    # --- 8. æ–°å¢ï¼šæ‰“å°æ€»æ—¶é—´è®¡æ—¶ç»“æœ ---
    print("\n" + "="*80)
    print("ğŸš€ æ¨¡å‹æ€»ä½“æ€§èƒ½ (ç«¯åˆ°ç«¯)")
    print("="*80)
    print(f"{'æŒ‡æ ‡':<40} {'æ•°å€¼':<20}")
    print("-"*80)
    print(f"{'æ€»è€—æ—¶ (ç§’)':<40} {total_elapsed_time:<20.4f}")
    print(f"{'å¹³å‡æ¯æ¬¡å‰å‘ä¼ æ’­è€—æ—¶ (æ¯«ç§’)':<40} {avg_time_per_run_ms:<20.3f}")
    print(f"{'æ¨¡å‹ååç‡ (FPS)':<40} {fps:<20.2f}")
    print("="*80)

    # --- (å¯é€‰) æ‰“å°æ¨¡å‹æ€»å‚æ•° ---
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\næ¨¡å‹æ€»å¯è®­ç»ƒå‚æ•°: {total_params / 1e6:.2f} M")


if __name__ == "__main__":
    main()